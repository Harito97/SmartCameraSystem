{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'resnet50_fc512', 'se_resnet50', 'se_resnet50_fc512', 'se_resnet101', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'densenet121_fc512', 'inceptionresnetv2', 'inceptionv4', 'xception', 'resnet50_ibn_a', 'resnet50_ibn_b', 'nasnsetmobile', 'mobilenetv2_x1_0', 'mobilenetv2_x1_4', 'shufflenet', 'squeezenet1_0', 'squeezenet1_0_fc512', 'squeezenet1_1', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'mudeep', 'resnet50mid', 'hacnn', 'pcb_p6', 'pcb_p4', 'mlfn', 'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0', 'osnet_ain_x1_0', 'osnet_ain_x0_75', 'osnet_ain_x0_5', 'osnet_ain_x0_25']\n"
     ]
    }
   ],
   "source": [
    "from torchreid import models\n",
    "models.show_avai_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding boxes from one picture\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"models/yolov3-tiny.weights\", \"models/yolov3-tiny.cfg\")\n",
    "classes = []\n",
    "with open(\"models/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    \n",
    "def detect_person(image):\n",
    "    # image must be cv2.imread\n",
    "    # Get image dimensions\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Preprocess image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Get output layer names\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "    # Forward pass\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    # List to store bounding boxes\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # Loop over detections\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:  # Class ID 0 is 'person'\n",
    "                center_x = int(obj[0] * width)\n",
    "                center_y = int(obj[1] * height)\n",
    "                w = int(obj[2] * width)\n",
    "                h = int(obj[3] * height)\n",
    "\n",
    "                # Bounding box coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                bounding_boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(368, 346, 577, 709), (112, 393, 322, 714), (591, 402, 810, 706)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image_path = \"img1.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "# Get bounding boxes\n",
    "boxes = detect_person(image)\n",
    "print(boxes)\n",
    "\n",
    "# Draw bounding boxes on the image\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "# Display the result\n",
    "cv2.namedWindow('Person Detection', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Person Detection\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def detect_person(image_path):\n",
    "#     # Đọc ảnh từ đường dẫn\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     # Đọc pre-trained YOLO model\n",
    "#     net = cv2.dnn.readNet('models/yolov3-tiny.weights', 'models/yolov3-tiny.cfg')\n",
    "\n",
    "#     # Load các tên lớp (coco.names) và các cấu hình của mô hình (yolov3.cfg)\n",
    "#     with open('models/coco.names', 'r') as f:\n",
    "#         classes = f.read().strip().split('\\n')\n",
    "\n",
    "#     # Chuyển đổi ảnh thành blob để sử dụng với mô hình YOLO\n",
    "#     blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "#     net.setInput(blob)\n",
    "\n",
    "#     # Lấy các tên lớp và các điểm tương ứng từ mô hình\n",
    "#     layer_names = net.getUnconnectedOutLayersNames()\n",
    "#     outputs = net.forward(layer_names)\n",
    "\n",
    "#     # Xác định các bounding box, điểm confidence và lớp dự đoán\n",
    "#     boxes = []\n",
    "#     confidences = []\n",
    "#     class_ids = []\n",
    "\n",
    "#     for output in outputs:\n",
    "#         for detection in output:\n",
    "#             scores = detection[5:]\n",
    "#             class_id = np.argmax(scores)\n",
    "#             confidence = scores[class_id]\n",
    "\n",
    "#             if confidence > 0.5 and class_id == classes.index('person'):\n",
    "#                 # Lấy tọa độ của bounding box\n",
    "#                 box = detection[0:4] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "#                 (center_x, center_y, width, height) = box.astype('int')\n",
    "\n",
    "#                 # Chuyển đổi tọa độ về góc trên trái của bounding box\n",
    "#                 x = int(center_x - (width / 2))\n",
    "#                 y = int(center_y - (height / 2))\n",
    "\n",
    "#                 boxes.append([x, y, int(width), int(height)])\n",
    "#                 confidences.append(float(confidence))\n",
    "#                 class_ids.append(class_id)\n",
    "\n",
    "#     # Áp dụng non-maximum suppression để loại bỏ các bounding box trùng lặp\n",
    "#     indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "#     # Tạo danh sách bounding boxes cuối cùng\n",
    "#     final_boxes = []\n",
    "#     for i in indices:\n",
    "#         i = i[0]\n",
    "#         final_boxes.append(boxes[i])\n",
    "\n",
    "#     return final_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import torchreid\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Tạo một mô hình ReID\n",
    "model = torchreid.models.build_model(\n",
    "    name='resnet152',\n",
    "    num_classes=1000,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Đọc ảnh và chuyển đổi sang định dạng PyTorch\n",
    "    image = cv2.imread(image_path)\n",
    "    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image = F.to_tensor(F.resize(image, (256, 128))).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     # Đọc ảnh và chuyển đổi sang định dạng PyTorch\n",
    "#     image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "#     image = F.to_tensor(F.resize(image, (256, 128))).unsqueeze(0)\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ảnh 1 và 2 là hai ảnh của cùng một nhóm người chụp từ góc khác nhau\n",
    "image1 = preprocess_image('0.jpg')\n",
    "image2 = preprocess_image('1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image1.shape)\n",
    "print(type(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes1 = detect_person(cv2.imread('0.jpg'))\n",
    "bounding_boxes2 = detect_person(cv2.imread('1.jpg'))\n",
    "print(bounding_boxes1)\n",
    "print(bounding_boxes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = []\n",
    "# Loop over bounding boxes\n",
    "for bbox in bounding_boxes1:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi_image = image1[:, y1:y2, x1:x2]\n",
    "\n",
    "    # Resize the ROI to the required input size of the model\n",
    "    resized_roi = F.resize(roi_image, (128, 256))\n",
    "\n",
    "    # Convert resized_roi to PyTorch tensor\n",
    "    input_tensor = resized_roi.unsqueeze(0).float()\n",
    "\n",
    "    # Trích xuất đặc trưng từ ROI\n",
    "    object_feature = model(input_tensor)['global_feat'].detach().numpy()\n",
    "    features1.append(object_feature)\n",
    "\n",
    "features2 = []\n",
    "# Loop over bounding boxes\n",
    "for bbox in bounding_boxes2:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi_image = image2[:, y1:y2, x1:x2]\n",
    "\n",
    "    # Resize the ROI to the required input size of the model\n",
    "    resized_roi = F.resize(roi_image, (128, 256))\n",
    "\n",
    "    # Convert resized_roi to PyTorch tensor\n",
    "    input_tensor = resized_roi.unsqueeze(0).float()\n",
    "\n",
    "    # Trích xuất đặc trưng từ ROI\n",
    "    object_feature = model(input_tensor)['global_feat'].detach().numpy()\n",
    "    features2.append(object_feature)\n",
    "\n",
    "# features1 và features2 là danh sách các đặc trưng của từng đối tượng trong ảnh 1 và 2\n",
    "print(len(features1), len(features2))\n",
    "\n",
    "# Tính cosine similarity giữa hai vector đặc trưng\n",
    "similarity = 1 - cosine(features1[0], features2[0])\n",
    "\n",
    "# Nếu similarity lớn hơn một ngưỡng nào đó, có thể coi đó là cùng một người\n",
    "threshold = 0.8\n",
    "if similarity > threshold:\n",
    "    print(\"Cùng một người!\")\n",
    "else:\n",
    "    print(\"Không phải cùng một người.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gdown tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra kích thước của image1\n",
    "print(\"image1 Shape:\", image1.shape)\n",
    "\n",
    "for bbox in bounding_boxes1:\n",
    "    x1, y1, x2, y2 = bbox\n",
    "\n",
    "    # Kiểm tra giá trị của y1, y2, x1, x2\n",
    "    print(\"y1:\", y1)\n",
    "    print(\"y2:\", y2)\n",
    "    print(\"x1:\", x1)\n",
    "    print(\"x2:\", x2)\n",
    "\n",
    "    # Truy cập phần của image1\n",
    "    roi_image = image1[:, y1:y2, x1:x2]\n",
    "\n",
    "    # Kiểm tra giá trị và kích thước của roi_image\n",
    "    print(\"roi_image Value:\", roi_image)\n",
    "    print(\"roi_image Shape:\", roi_image.shape)\n",
    "\n",
    "    # Resize the ROI to the required input size of the model\n",
    "    resized_roi = F.resize(roi_image, (128, 256))\n",
    "\n",
    "    # Kiểm tra giá trị và kích thước của resized_roi\n",
    "    print(\"resized_roi Value:\", resized_roi)\n",
    "    print(\"resized_roi Shape:\", resized_roi.shape)\n",
    "\n",
    "    # Convert resized_roi to PyTorch tensor\n",
    "    input_tensor = resized_roi.unsqueeze(0).float()\n",
    "\n",
    "    # Kiểm tra giá trị và kích thước của input_tensor\n",
    "    print(\"input_tensor Value:\", input_tensor)\n",
    "    print(\"input_tensor Shape:\", input_tensor.shape)\n",
    "\n",
    "    # ... (các dòng mã khác)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
