{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harito/venv/py/lib/python3.11/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "yolo_model = YOLO('yolov8x.pt')\n",
    "\n",
    "import torchreid\n",
    "reid_model = torchreid.models.build_model(\n",
    "    name='mudeep',\n",
    "    num_classes=1000,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_path, yolo_model):\n",
    "    # Load ảnh\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Sử dụng YOLO để xác định vùng chứa object\n",
    "    results = yolo_model(img)\n",
    "\n",
    "    for r in results:\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "        im.show()  # show image\n",
    "        # im.save('results.jpg')  # save image\n",
    "\n",
    "    # Lấy thông tin về object và bounding box\n",
    "    boxes = results[0].boxes.xywh   # tensor\n",
    "\n",
    "    # Cắt và lưu các vùng chứa object\n",
    "    object_images = []\n",
    "    for box in boxes:\n",
    "        xmin, ymin, width, height = map(int, box)\n",
    "        xmax = xmin + width\n",
    "        ymax = ymin + height\n",
    "        object_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "        object_images.append(object_img)\n",
    "\n",
    "    return object_images\n",
    "\n",
    "# def extract_features(image, reid_model):\n",
    "#     \"\"\"image is <class 'PIL.Image.Image'>\"\"\"\n",
    "#     # Load ảnh và tiền xử lý nó để phù hợp với mô hình\n",
    "#     img = image.convert('RGB') # make 3 channels\n",
    "#     preprocess = transforms.Compose([\n",
    "#         transforms.Resize((256, 128)), # make size (256, 128)\n",
    "#         transforms.ToTensor(),  # make tensor (3, 256, 128)\n",
    "#     ])\n",
    "#     img = preprocess(img).unsqueeze(0)\n",
    "\n",
    "#     # Đưa ảnh qua mô hình để lấy đặc trưng\n",
    "#     with torch.no_grad():\n",
    "#         # features = reid_model.featuremaps(img)\n",
    "#         features = reid_model.forward(img)\n",
    "\n",
    "#     return features\n",
    "\n",
    "def extract_features(image, reid_model):\n",
    "    \"\"\"\n",
    "    Extract features from an image using a reid_model.\n",
    "\n",
    "    Parameters:\n",
    "        image (PIL.Image.Image): Input image.\n",
    "        reid_model: TorchReID model.\n",
    "\n",
    "    Returns:\n",
    "        features (torch.Tensor): Extracted features.\n",
    "    \"\"\"\n",
    "    # Preprocess the image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 128)),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_tensor = preprocess(image.convert('RGB'))\n",
    "    img_tensor = torch.unsqueeze(img_tensor, 0)  # Add batch dimension\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    reid_model.eval()\n",
    "\n",
    "    # Disable gradient computation to speed up the process\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to extract features\n",
    "        features = reid_model.forward(img_tensor)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 320x640 4 persons, 1 car, 2273.5ms\n",
      "Speed: 4.3ms preprocess, 2273.5ms inference, 2.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2625.8ms\n",
      "Speed: 4.2ms preprocess, 2625.8ms inference, 2.1ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(loupe:31216): Adwaita-WARNING **: 20:45:37.012: AdwToolbarView 0x55f2903bb450 exceeds LpWindow width: requested 355 px, 317 px available\n",
      "\n",
      "(loupe:31216): GLib-GObject-CRITICAL **: 20:46:30.696: g_object_weak_unref: couldn't find weak ref 0x7fa98ad16970((nil))\n",
      "\n",
      "(loupe:31216): GLib-GObject-CRITICAL **: 20:47:14.331: g_object_weak_unref: couldn't find weak ref 0x7fa98ad16970((nil))\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn đến ảnh 1 và ảnh 2\n",
    "image_path_1 = '0.jpg'\n",
    "image_path_2 = '1.jpg'\n",
    "# Dùng YOLO để xác định vùng chứa object trên ảnh 1 và ảnh 2\n",
    "object_images_1 = detect_objects(image_path_1, yolo_model)\n",
    "object_images_2 = detect_objects(image_path_2, yolo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "(845, 594)\n",
      "<PIL.Image.Image image mode=RGB size=845x594 at 0x7F8CF0CA6C50>\n"
     ]
    }
   ],
   "source": [
    "print(type(object_images_1[0]))\n",
    "print(object_images_1[0].size)\n",
    "print(object_images_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trích xuất vector đặc trưng từ reid_model cho từng vùng chứa object\n",
    "features_1 = [extract_features(img, reid_model) for img in object_images_1]\n",
    "features_2 = [extract_features(img, reid_model) for img in object_images_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số các features 1 và số các features 2: 5 4\n",
      "Kích thước của 1 feature của 1 vùng ảnh - 1 object: torch.Size([1, 4096])\n",
      "Thông tin 1 feature: tensor([[0.0191, 0.0000, 0.0000,  ..., 0.0251, 0.0059, 0.0000]])\n",
      "Kích thước 1 feature - tensor có 1 ảnh xử lý khi gọi mô hình: 1\n",
      "Kiểu dữ liệu của feature: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Tìm hiểu về cấu trúc dữ liệu\n",
    "print(\"Số các features 1 và số các features 2:\", len(features_1), len(features_2))\n",
    "print(\"Kích thước của 1 feature của 1 vùng ảnh - 1 object:\", features_1[0].shape)\n",
    "# print(\"Kích thước của 1 feature của 1 vùng ảnh - 1 object:\", features_2[0].shape)\n",
    "# for feature in features_1:\n",
    "#     print(\"Thông tin 1 feature:\", feature)\n",
    "#     print(\"Kích thước 1 feature:\", len(feature))\n",
    "#     print(\"Kiểu dữ liệu của feature:\", type(feature))\n",
    "\n",
    "print(\"Thông tin 1 feature:\", features_1[0])\n",
    "print(\"Kích thước 1 feature - tensor có 1 ảnh xử lý khi gọi mô hình:\", len(features_1[0]))\n",
    "print(\"Kiểu dữ liệu của feature:\", type(features_1[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([1, 4096])\n",
      "tensor([[0.0191, 0.0000, 0.0000,  ..., 0.0251, 0.0059, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(type(features_1))\n",
    "print(features_1[0].shape)\n",
    "print(features_1[0])\n",
    "# print(features_1[0][0]) # có 256 phần tử như này\n",
    "# print(features_1[0][0][0])  # có 16 phần tử như này\n",
    "# print(features_1[0][0][1])  # có 16 phần tử như này\n",
    "# print(features_1[0][0][0][0])  # có 8 phần tử như này"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harito/venv/py/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/harito/venv/py/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 remote, 2604.5ms\n",
      "Speed: 10.0ms preprocess, 2604.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 remote, 3737.5ms\n",
      "Speed: 5.5ms preprocess, 3737.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 3583.1ms\n",
      "Speed: 6.4ms preprocess, 3583.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "features1 and features2: 0.8383724093437195\n",
      "features1 is similar to features2.\n",
      "features1 and features3: 0.6463674306869507\n",
      "features3 and features2: 0.5626075267791748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.049: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.060: AdwToolbarView 0x560ac407e800 exceeds LpWindow width: requested 355 px, 322 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.072: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.090: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.108: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.127: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.145: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.162: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:21.179: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:22.561: AdwToolbarView 0x560ac407e800 exceeds LpWindow width: requested 355 px, 322 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:23.636: AdwToolbarView 0x560ac407e800 exceeds LpWindow width: requested 355 px, 322 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:26.280: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:27.485: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:30.834: AdwToolbarView 0x560ac407e800 exceeds LpWindow width: requested 355 px, 322 px available\n",
      "\n",
      "(loupe:34679): Adwaita-WARNING **: 21:26:30.842: AdwToolbarView 0x560ac66f84f0 exceeds LpWindow width: requested 355 px, 316 px available\n",
      "\n",
      "(loupe:34679): GLib-GObject-CRITICAL **: 21:27:35.649: g_object_weak_unref: couldn't find weak ref 0x7f8872f16970((nil))\n",
      "\n",
      "(loupe:34679): GLib-GObject-CRITICAL **: 21:27:36.524: g_object_weak_unref: couldn't find weak ref 0x7f8872f16970((nil))\n",
      "\n",
      "(loupe:34679): GLib-GObject-CRITICAL **: 21:27:37.682: g_object_weak_unref: couldn't find weak ref 0x7f8872f16970((nil))\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng mô hình đã có hoặc đào tạo mô hình phù hợp với đối tượng của bạn\n",
    "# Ví dụ sử dụng torchvision.models\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def extract_features(image, model):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = torch.unsqueeze(input_tensor, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Sử dụng một pre-trained model như ResNet\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.eval()\n",
    "\n",
    "img1 = Image.open(\"calcu_1.jpg\").convert('RGB')\n",
    "img2 = Image.open(\"calcu_2.jpg\").convert('RGB')\n",
    "img3 = Image.open(\"box.jpg\").convert('RGB')\n",
    "# Dùng YOLO để xác định vùng chứa object trên ảnh 1 và ảnh 2\n",
    "object_images_1 = detect_objects(img1, yolo_model)\n",
    "object_images_2 = detect_objects(img2, yolo_model)\n",
    "object_images_3 = detect_objects(img3, yolo_model)\n",
    "features1 = extract_features(img1, resnet_model)\n",
    "features2 = extract_features(img2, resnet_model)\n",
    "features3 = extract_features(img3, resnet_model)\n",
    "\n",
    "# So sánh biểu diễn feature\n",
    "# cosine_similarity = torch.nn.functional.cosine_similarity(features1, features2)\n",
    "\n",
    "threshold = 0.8\n",
    "similarity_score = cosine_similarity(features1, features2)\n",
    "# In kết quả\n",
    "print(f'features1 and features2: {similarity_score[0, 0]}')\n",
    "if similarity_score >= threshold:\n",
    "    print(\"features1 is similar to features2.\")\n",
    "similarity_score = cosine_similarity(features1, features3)\n",
    "# In kết quả\n",
    "print(f'features1 and features3: {similarity_score[0, 0]}')\n",
    "if similarity_score >= threshold:\n",
    "    print(\"features1 is similar to features3.\")\n",
    "similarity_score = cosine_similarity(features3, features2)\n",
    "# In kết quả\n",
    "print(f'features3 and features2: {similarity_score[0, 0]}')\n",
    "if similarity_score >= threshold:\n",
    "    print(\"features3 is similar to features2.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harito/venv/py/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/harito/venv/py/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 4 persons, 1 car, 2169.3ms\n",
      "Speed: 6.8ms preprocess, 2169.3ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 2676.6ms\n",
      "Speed: 12.8ms preprocess, 2676.6ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(loupe:37928): GLib-GObject-CRITICAL **: 21:46:16.133: g_object_weak_unref: couldn't find weak ref 0x7f6602d16970((nil))\n",
      "\n",
      "(loupe:38075): GLib-GObject-CRITICAL **: 21:46:19.958: g_object_weak_unref: couldn't find weak ref 0x7f5d0c916970((nil))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Sử dụng một pre-trained model như ResNet\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.eval()\n",
    "\n",
    "def detect_objects(image, yolo_model):\n",
    "\n",
    "    # Sử dụng YOLO để xác định vùng chứa object\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    for r in results:\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "        im.show()  # show image\n",
    "        # im.save('results.jpg')  # save image\n",
    "\n",
    "    # Lấy thông tin về object và bounding box\n",
    "    boxes = results[0].boxes.xywh   # tensor\n",
    "\n",
    "    # Cắt và lưu các vùng chứa object\n",
    "    object_images = []\n",
    "    for box in boxes:\n",
    "        xmin, ymin, width, height = map(int, box)\n",
    "        xmax = xmin + width\n",
    "        ymax = ymin + height\n",
    "        object_img = image.crop((xmin, ymin, xmax, ymax))\n",
    "        object_images.append(object_img)\n",
    "\n",
    "    return object_images\n",
    "\n",
    "def extract_features(image, model):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = torch.unsqueeze(input_tensor, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Load ảnh\n",
    "img1 = Image.open('0.jpg').convert('RGB')\n",
    "img2 = Image.open('1.jpg').convert('RGB')\n",
    "# Dùng YOLO để xác định vùng chứa object trên ảnh 1 và ảnh 2\n",
    "object_images_1 = detect_objects(img1, yolo_model)\n",
    "object_images_2 = detect_objects(img2, yolo_model)\n",
    "\n",
    "features_1 = [extract_features(img, resnet_model) for img in object_images_1]\n",
    "features_2 = [extract_features(img, resnet_model) for img in object_images_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_1[0] and features_2[0]: 0.6221237182617188\n",
      "features_1[0] and features_2[1]: 0.49044182896614075\n",
      "features_1[0] and features_2[2]: 0.4481508731842041\n",
      "features_1[0] and features_2[3]: 0.5429463386535645\n",
      "features_1[1] and features_2[0]: 0.7480502724647522\n",
      "features_1[1] and features_2[1]: 0.7209721207618713\n",
      "features_1[1] and features_2[2]: 0.5981481075286865\n",
      "features_1[1] and features_2[3]: 0.7117047905921936\n",
      "features_1[2] and features_2[0]: 0.6698352098464966\n",
      "features_1[2] and features_2[1]: 0.6257959008216858\n",
      "features_1[2] and features_2[2]: 0.560143768787384\n",
      "features_1[2] and features_2[3]: 0.6474456191062927\n",
      "features_1[3] and features_2[0]: 0.6051077842712402\n",
      "features_1[3] and features_2[1]: 0.5055397152900696\n",
      "features_1[3] and features_2[2]: 0.538964033126831\n",
      "features_1[3] and features_2[3]: 0.6177788972854614\n",
      "features_1[4] and features_2[0]: 0.8012514710426331\n",
      "features_1[4] is similar to features_2[0].\n",
      "features_1[4] and features_2[1]: 0.732859194278717\n",
      "features_1[4] and features_2[2]: 0.670677900314331\n",
      "features_1[4] and features_2[3]: 0.6738163828849792\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "# threshold = 1 / sqrt(2)\n",
    "threshold = 0.75\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(features_1)):\n",
    "    for j in range(len(features_2)):\n",
    "        # features_1 và features_2 là các đặc trưng được trích xuất từ hai ảnh\n",
    "        similarity_score = cosine_similarity(features_1[i], features_2[j])\n",
    "\n",
    "        # In kết quả\n",
    "        print(f'features_1[{i}] and features_2[{j}]: {similarity_score[0, 0]}')\n",
    "\n",
    "        if similarity_score >= threshold:\n",
    "            print(f\"features_1[{i}] is similar to features_2[{j}].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_1[1] is similar to features_2[0] with similarity: [[    0.74805]]\n",
      "features_1[4] is similar to features_2[0] with similarity: [[    0.80125]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.73\n",
    "\n",
    "max_similarity_pairs = []\n",
    "\n",
    "for i in range(len(features_1)):\n",
    "    max_similarity = -1  # Điểm tương đồng lớn nhất\n",
    "    max_index = -1  # Chỉ số của features_2 có điểm tương đồng lớn nhất\n",
    "\n",
    "    for j in range(len(features_2)):\n",
    "        similarity = cosine_similarity(features_1[i], features_2[j])\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_index = j\n",
    "\n",
    "    if max_similarity > threshold:\n",
    "        max_similarity_pairs.append((i, max_index, max_similarity))\n",
    "\n",
    "# In kết quả\n",
    "for pair in max_similarity_pairs:\n",
    "    print(f\"features_1[{pair[0]}] is similar to features_2[{pair[1]}] with similarity: {pair[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
