{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import cv2, numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Initialise the object tracker class\n",
    "object_tracker = DeepSort()\n",
    "detector = YOLO('yolov8n.pt')\n",
    "cv2.namedWindow('DeepSort', cv2.WINDOW_NORMAL)\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    results = detector.score_frame(img)\n",
    "    img,detections = detector.plot_boxes(results, img, height=img.shape[0], width=img.shape[1], confidence=0.5)\n",
    "\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img) \n",
    "    # NOTE: Bounding box expects to be a list of detections, each in tuples of ([left, top, w, h], confidence, detection class)\n",
    "    \n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "\n",
    "        bbox = ltrb\n",
    "\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,0,255),2)\n",
    "        cv2.putText(img, \"ID: \" + str(track_id), (int(bbox[0]),int(bbox[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end-start\n",
    "    fps = 1/totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    cv2.imshow('DeepSort', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/harito/venv/py/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 tv, 248.5ms\n",
      "Speed: 16.7ms preprocess, 248.5ms inference, 11.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'to_xyah'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Args:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        path (str): The path to the image file.\n        names (dict): A dictionary of class names.\n        boxes (torch.tensor, optional): A 2D tensor of bounding box coordinates for each detection.\n        masks (torch.tensor, optional): A 3D tensor of detection masks, where each mask is a binary image.\n        probs (torch.tensor, optional): A 1D tensor of probabilities of each class for classification task.\n        keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n\n    Attributes:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        orig_shape (tuple): The original image shape in (height, width) format.\n        boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n        masks (Masks, optional): A Masks object containing the detection masks.\n        probs (Probs, optional): A Probs object containing probabilities of each class for classification task.\n        keypoints (Keypoints, optional): A Keypoints object containing detected keypoints for each object.\n        speed (dict): A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.\n        names (dict): A dictionary of class names.\n        path (str): The path to the image file.\n        _keys (tuple): A tuple of attribute names for non-empty attributes.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dets \u001b[38;5;241m=\u001b[39m detector(frame1)\n\u001b[0;32m---> 19\u001b[0m online_targets \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCam\u001b[39m\u001b[38;5;124m'\u001b[39m, frame1)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/venv/py/lib/python3.11/site-packages/deepsort/tracker.py:78\u001b[0m, in \u001b[0;36mDeepSortTracker.update\u001b[0;34m(self, detections)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks[track_idx]\u001b[38;5;241m.\u001b[39mmark_missed()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection_idx \u001b[38;5;129;01min\u001b[39;00m unmatched_detections:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initiate_track\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdetection_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_deleted()]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Update distance metric.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/py/lib/python3.11/site-packages/deepsort/tracker.py:131\u001b[0m, in \u001b[0;36mDeepSortTracker._initiate_track\u001b[0;34m(self, detection)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initiate_track\u001b[39m(\u001b[38;5;28mself\u001b[39m, detection):\n\u001b[0;32m--> 131\u001b[0m     mean, covariance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkf\u001b[38;5;241m.\u001b[39minitiate(\u001b[43mdetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_xyah\u001b[49m())\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks\u001b[38;5;241m.\u001b[39mappend(Track(mean, covariance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_init, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_age, detection\u001b[38;5;241m.\u001b[39mfeature))\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venv/py/lib/python3.11/site-packages/ultralytics/utils/__init__.py:153\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Results' object has no attribute 'to_xyah'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Args:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        path (str): The path to the image file.\n        names (dict): A dictionary of class names.\n        boxes (torch.tensor, optional): A 2D tensor of bounding box coordinates for each detection.\n        masks (torch.tensor, optional): A 3D tensor of detection masks, where each mask is a binary image.\n        probs (torch.tensor, optional): A 1D tensor of probabilities of each class for classification task.\n        keypoints (List[List[float]], optional): A list of detected keypoints for each object.\n\n    Attributes:\n        orig_img (numpy.ndarray): The original image as a numpy array.\n        orig_shape (tuple): The original image shape in (height, width) format.\n        boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n        masks (Masks, optional): A Masks object containing the detection masks.\n        probs (Probs, optional): A Probs object containing probabilities of each class for classification task.\n        keypoints (Keypoints, optional): A Keypoints object containing detected keypoints for each object.\n        speed (dict): A dictionary of preprocess, inference, and postprocess speeds in milliseconds per image.\n        names (dict): A dictionary of class names.\n        path (str): The path to the image file.\n        _keys (tuple): A tuple of attribute names for non-empty attributes.\n    "
     ]
    }
   ],
   "source": [
    "from deepsort.tracker import DeepSortTracker\n",
    "# from deepsort.detection import Detection\n",
    "from ultralytics import YOLO\n",
    "import cv2 \n",
    "\n",
    "# detector = Detection()\n",
    "detector = YOLO('yolov8n.pt')\n",
    "tracker = DeepSortTracker()\n",
    "\n",
    "cv2.namedWindow('Cam', cv2.WINDOW_NORMAL)\n",
    "cam1 = '/home/harito/Videos/Cam1.mp4'\n",
    "cap1 = cv2.VideoCapture(cam1)\n",
    "\n",
    "while cap1.isOpened():\n",
    "    ret1, frame1 = cap1. read()\n",
    "    if not ret1:\n",
    "        break\n",
    "    dets = detector(frame1)\n",
    "    online_targets = tracker.update(dets)\n",
    "    cv2.imshow('Cam', frame1)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "            \n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
