{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from App import App\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "app = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('data_test/per1_1.jpg')\n",
    "# object_images, results = app.detect_objects(image1)\n",
    "results = app.yolo_model(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes[0].xywh[0][1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Forward pass for both images\n",
    "        out1 = self.base_model(x1)\n",
    "        out2 = self.base_model(x2)\n",
    "        return out1, out2\n",
    "\n",
    "# # Create a base model (ResNet-18 in this case)\n",
    "# base_model = models.resnet18(pretrained=True)\n",
    "# # Modify the final fully connected layer to output a suitable dimension\n",
    "# base_model.fc = nn.Linear(base_model.fc.in_features, 256)\n",
    "\n",
    "# # Create a Siamese Network\n",
    "# siamese_net = SiameseNetwork(base_model)\n",
    "\n",
    "# Create a base model (ResNet-18 in this case)\n",
    "base_model = models.resnet18(pretrained=True)\n",
    "base_model.fc = nn.Linear(base_model.fc.in_features, 256)\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    base_model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Create a Siamese Network with the quantized model\n",
    "siamese_net = SiameseNetwork(quantized_model)\n",
    "\n",
    "# Example forward pass with two images\n",
    "# image1 = torch.randn(1, 3, 224, 224)  # replace with your image dimensions\n",
    "# image2 = torch.randn(1, 3, 224, 224)  # replace with your image dimensions\n",
    "\n",
    "# Example usage with OpenCV images\n",
    "image_path1 = \"data_test/test_reid/tower0_0.jpg\"\n",
    "image_path2 = \"data_test/test_reid/tower1_0.jpg\"\n",
    "\n",
    "# Read images using OpenCV\n",
    "image1 = cv2.imread(image_path1)\n",
    "image2 = cv2.imread(image_path2)\n",
    "\n",
    "# Check if images are read successfully\n",
    "if image1 is None or image2 is None:\n",
    "    print(\"Error: One or both images could not be read.\")\n",
    "else:\n",
    "    # Convert OpenCV images to PyTorch tensors without PIL Image\n",
    "    def preprocess_image(image):\n",
    "        # Convert BGR to RGB\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Normalize to the range [0, 1]\n",
    "        image_tensor = torch.from_numpy(image_rgb / 255.0).permute(2, 0, 1).float()\n",
    "        # Add batch dimension\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "        return image_tensor\n",
    "\n",
    "    # Preprocess images\n",
    "    input_tensor1 = preprocess_image(image1)\n",
    "    input_tensor2 = preprocess_image(image2)\n",
    "\n",
    "    # Example forward pass with two images\n",
    "    embedding1, embedding2 = siamese_net(input_tensor1, input_tensor2)\n",
    "\n",
    "    # Compute the similarity score (cosine similarity in this case)\n",
    "    similarity_score = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    print(f\"Similarity Score: {similarity_score.item()}\")\n",
    "    # Resnet50 đang là 43.6s\n",
    "    # Resnet18 đang là 17.5s\n",
    "    # Sử dụng thêm Quantom đang là 11.0s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def color_histogram_similarity(image1, image2):\n",
    "    hist1 = cv2.calcHist([image1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    hist2 = cv2.calcHist([image2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "\n",
    "    # Normalize histograms\n",
    "    hist1 = cv2.normalize(hist1, hist1).flatten()\n",
    "    hist2 = cv2.normalize(hist2, hist2).flatten()\n",
    "\n",
    "    # Clip values to ensure they are within valid range\n",
    "    hist1 = np.clip(hist1, 0, 1)\n",
    "    hist2 = np.clip(hist2, 0, 1)\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Display histograms\n",
    "    plt.plot(hist1, color='red', label='Image 1')\n",
    "    plt.plot(hist2, color='blue', label='Image 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Calculate histogram intersection similarity\n",
    "    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_INTERSECT)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path1 = \"data_test/test_reid/tower0_0.jpg\"\n",
    "image_path2 = \"data_test/test_reid/tower1_0.jpg\"\n",
    "\n",
    "image1 = cv2.imread(image_path1)\n",
    "image2 = cv2.imread(image_path2)\n",
    "\n",
    "# Resize images for consistency (optional)\n",
    "image1 = cv2.resize(image1, (224, 224))\n",
    "image2 = cv2.resize(image2, (224, 224))\n",
    "\n",
    "# Calculate color histogram similarity\n",
    "similarity_score = color_histogram_similarity(image1, image2)\n",
    "print(f\"Color Histogram Similarity Score: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def are_images_equal(image1, image2):\n",
    "    # Đọc ảnh\n",
    "    img1 = cv2.imread(image1)\n",
    "    img2 = cv2.imread(image2)\n",
    "\n",
    "    # Chuyển đổi ảnh sang không gian màu HSV\n",
    "    hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV)\n",
    "    hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Tính histogram của ảnh\n",
    "    hist1 = cv2.calcHist([hsv1], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "    hist2 = cv2.calcHist([hsv2], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "\n",
    "    # Chuẩn hóa histogram\n",
    "    cv2.normalize(hist1, hist1, 0, 1, cv2.NORM_MINMAX)\n",
    "    cv2.normalize(hist2, hist2, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Tính độ tương đồng giữa hai histogram\n",
    "    similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "    # Xác định ngưỡng để quyết định hai ảnh có chụp cùng đối tượng hay không\n",
    "    threshold = 0.8  # Ngưỡng có thể điều chỉnh\n",
    "\n",
    "    if similarity > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Đường dẫn của hai ảnh cần so sánh\n",
    "image_path1 = \"data_test/test_reid/tower0_0.jpg\"\n",
    "image_path2 = \"data_test/test_reid/tower1_0.jpg\"\n",
    "\n",
    "# Kiểm tra xem hai ảnh có chụp cùng đối tượng hay không\n",
    "result = are_images_equal(image_path1, image_path2)\n",
    "\n",
    "if result:\n",
    "    print(\"Hai ảnh chụp cùng đối tượng.\")\n",
    "else:\n",
    "    print(\"Hai ảnh không chụp cùng đối tượng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1 = '/home/harito/Videos/Cam1.mp4'\n",
    "cam2 = '/home/harito/Videos/Cam2.mp4'\n",
    "\n",
    "cap1 = cv2.VideoCapture(cam1)\n",
    "cap2 = cv2.VideoCapture(cam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret1, frame1 = cap1.read()\n",
    "ret2, frame2 = cap2.read()\n",
    "real_frame = cv2.hconcat([frame1, frame2])\n",
    "\n",
    "frame1_detect = app.detect_objects(frame1)\n",
    "frame2_detect = app.detect_objects(frame2)\n",
    "frame1_detect = cv2.cvtColor(np.array(app.draw_picture_detect(frame1_detect[1])), cv2.COLOR_RGB2BGR)\n",
    "frame2_detect = cv2.cvtColor(np.array(app.draw_picture_detect(frame2_detect[1])), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "detect_frame = cv2.hconcat([frame1_detect, frame2_detect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_frame.shape)\n",
    "print(detect_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1_detect = app.detect_objects(frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame1_detect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_objects = []\n",
    "# for box in frame1_detect[1][0].boxes:\n",
    "#     detected_objects.append(box.cls())\n",
    "# print(detected_objects)\n",
    "print(frame1_detect[1][0].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frame1_detect[1][0].boxes[0])\n",
    "type(frame1_detect[1][0].boxes[0].cls.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "face_db_regis = pickle.loads(open(\"database/face_dictionary.pkl\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(face_db_regis))\n",
    "print(type(face_db_regis['encodings']))\n",
    "print(face_db_regis['encodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting YOLO model for detect ...\n",
      "Set reid with histogram\n",
      "Setting the dictionary {name: vector feature} from load_data_base() ...\n",
      "Set face recognition model with face_recognition \n",
      "Setup done!\n",
      "{'Duc': <Main.List object at 0x7ff30bbdca50>, 'DucAnh': <Main.List object at 0x7ff318ba53d0>, 'Hai': <Main.List object at 0x7ff30d929150>, 'Thang': <Main.List object at 0x7ff30bbc1d10>}\n"
     ]
    }
   ],
   "source": [
    "from Main import Main\n",
    "main = Main()\n",
    "print(main.reid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0.32005  0.00028474           0 ...           0           0    0.001139]\n",
      " [          0           0           0 ...           0           0   0.0014237]\n",
      " [          0           0           0 ...           0           0   0.0019932]\n",
      " ...\n",
      " [          0           0           0 ...           0           0           0]\n",
      " [          0           0           0 ...           0           0           0]\n",
      " [          0           0           0 ...           0           0           0]]\n"
     ]
    }
   ],
   "source": [
    "print(main.reid_dict['Duc'].data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
